{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "import operator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data_raw.json\") as f:\n",
    "    rawData = json.loads(f.read())\n",
    "\n",
    "with open(\"negative.json\") as f:\n",
    "    negativeData = json.loads(f.read())\n",
    "\n",
    "rawData.extend(negativeData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "stopWords = set(stopwords.words(\"russian\"))\n",
    "badwords = [u'я', u'а', u'да', u'но', u'тебе', u'мне', u'ты', u'и', u'у', u'на', u'ща', u'ага',\n",
    "            u'так', u'там', u'какие', u'который', u'какая', u'туда', u'давай', u'короче', u'кажется', u'вообще',\n",
    "            u'ну', u'не', u'чет', u'неа', u'свои', u'наше', u'хотя', u'такое', u'например', u'кароч', u'как-то',\n",
    "            u'нам', u'хм', u'всем', u'нет', u'да', u'оно', u'своем', u'про', u'вы', u'м', u'тд',\n",
    "            u'вся', u'кто-то', u'что-то', u'вам', u'это', u'эта', u'эти', u'этот', u'прям', u'либо', u'как', u'мы',\n",
    "            u'просто', u'блин', u'очень', u'самые', u'твоем', u'ваша', u'кстати', u'вроде', u'типа', u'пока', u'ок',u'в',\n",
    "            u'б',u'г',u'д',u'е',u'ж',u'з',u'й',u'к',u'л',u'ф',u'н',u'о',u'п',u'р',u'с',u'т',u'ч',u'ц',u'ч',\n",
    "            u'ш',u'щ',u'ь',u'ъ',u'ы',u'э','ю']\n",
    "\n",
    "popluarCountries = [u'казахстан', u'россия',u'узбекистан',u'киргизия',\n",
    "                    u'сша',u'штаты',u'америка',u'китай',u'туркменистан',\n",
    "                    u'сирия',u'монголия',u'франция',u'англия',u'турция']\n",
    "\n",
    "KZCities = [u'алма-ата',u'алматы',u'караганда',u'караганды',\n",
    "            u'уральск',u'орал',u'усть-каменогорск',u'оскемен',\n",
    "            u'кокшетау',u'кокчетав',u'семей',u'семипалатинск',\n",
    "            u'тараз',u'шымкент',u'астана',u'павлодар',\n",
    "            u'актобе',u'атырау',u'актау',u'кызылорда',\n",
    "            u'петропавловск',u'талдыкорган',u'костанай',u'шу',\n",
    "            u'жезказган',u'байконур',u'туркестан',u'экибастуз',\n",
    "            u'астан',u'столица',u'город',u'столичный',u'рк',\n",
    "            u'жамбылский',u'алматинский',u'акмолинский',\n",
    "            u'западный',u'восточный',u'северный',u'южный',\n",
    "            u'центральный']\n",
    "\n",
    "Time = [u'январь',u'февраль',u'март',u'апрель',\n",
    "        u'май',u'июнь',u'июль',u'август',\n",
    "        u'сентябрь',u'октябрь',u'ноябрь',u'декабрь',\n",
    "        u'понедельник',u'вторник',u'среда',u'четверг',\n",
    "        u'пятница',u'суббота',u'воскресенье',u'день',\n",
    "        'завтра','сегодня','вчера']\n",
    "\n",
    "Numerals = [u'ноль',u'один',u'два',u'три',\n",
    "            u'четыре'u'пять',u'шесть',u'семь',\n",
    "            u'восемь',u'девять',u'десять',u'сто',\n",
    "            u'тысяча',u'миллион',u'миллиард',u'триллион']\n",
    "\n",
    "badwords.extend(popluarCountries)\n",
    "badwords.extend(KZCities)\n",
    "badwords.extend(Time)\n",
    "badwords.extend(Numerals)\n",
    "\n",
    "for word in badwords:\n",
    "    stopWords.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Lemmatisator(data):\n",
    "    lemma = m.lemmatize(data)\n",
    "    return ''.join(lemma)\n",
    "\n",
    "def stopwordsEleminator(data, negation=True):\n",
    "    for word in stopWords:\n",
    "        if(word == u'не' or word == u'ни') and (negation == True):\n",
    "            continue\n",
    "        if word in data:\n",
    "            for i in range(data.count(word)):\n",
    "                data.remove(word)\n",
    "    return data\n",
    "\n",
    "def dataCleaner(data, negationConcat = False): # False if clear all, True if clear and concatinate negations\n",
    "    cleanedData = re.sub(\"[^а-яА-ЯЁё]\",\" \", data) # leave only russian text\n",
    "    cleanedData = Lemmatisator(cleanedData)\n",
    "    cleanedData = cleanedData.lower().split()\n",
    "    cleanedData = stopwordsEleminator(cleanedData,negationConcat)\n",
    "    \n",
    "    if negationConcat == True:\n",
    "        for i in range (0, len(cleanedData)-1):\n",
    "            if(cleanedData[i] == u'не' or cleanedData[i] == u'ни'):\n",
    "                cleanedData[i+1]=('не'+cleanedData[i+1])\n",
    "        cleanedData = stopwordsEleminator(cleanedData,False)\n",
    "\n",
    "    return cleanedData\n",
    "\n",
    "def duplicateEleminator(data):\n",
    "    uniqueData = list()\n",
    "    cnt = 0\n",
    "    \n",
    "    for item in data:\n",
    "        if item not in uniqueData:\n",
    "            uniqueData.append(item)\n",
    "        else:\n",
    "            cnt+=1\n",
    "    print(\"Duplicates:{}\".format(cnt))        \n",
    "    return uniqueData        \n",
    "\n",
    "def duplicatedTextEleminator(data):\n",
    "    uniqueData = list()\n",
    "    cnt = 0\n",
    "    for item1 in data:\n",
    "        duplicate = False\n",
    "        for item2 in data:\n",
    "            if (item1[\"text\"] == item2[\"text\"]) and (item1[\"manual_sentiment\"] != item2[\"manual_sentiment\"]):\n",
    "                duplicate = True\n",
    "                cnt+=1\n",
    "        if duplicate == False:\n",
    "            uniqueData.append(item1)\n",
    "    print(\"Duplicates with simillar text:{}\".format(cnt))\n",
    "    return uniqueData        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(rawData)):\n",
    "    rawData[i][\"text\"] = dataCleaner(rawData[i][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates:4657\n"
     ]
    }
   ],
   "source": [
    "uniqueData = duplicateEleminator(rawData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates with simillar text:1414\n"
     ]
    }
   ],
   "source": [
    "preparedData = duplicatedTextEleminator(uniqueData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open (\"data_prepared.json\", \"w\", encoding='utf-8') as outfile: \n",
    "    outfile.write('[')\n",
    "    for i in range(0, len(preparedData)):\n",
    "        json.dump(preparedData[i], outfile, ensure_ascii=False)\n",
    "        if i+1 < len(preparedData):\n",
    "            outfile.write(',')\n",
    "        outfile.write('\\n')    \n",
    "    outfile.write(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
